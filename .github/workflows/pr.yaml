name: PR Checks

on:
  push:
    branches:
      - main
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]

# Make sure CI fails on all warnings, including Clippy lints
env:
  RUSTFLAGS: "-Dwarnings"

permissions:
  # Required for PR comments
  pull-requests: write
  # Required for storing benchmark results
  contents: write

jobs:
  check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: cargo install cargo-hack
        run: cargo +stable install cargo-hack --locked
      - name: cargo check
        run: cargo hack check --each-feature --no-dev-deps

  clippy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: cargo install cargo-hack
        run: cargo +stable install cargo-hack --locked
      - name: cargo clippy
        run: cargo hack clippy --each-feature --no-dev-deps && cargo hack clippy --lib -p slatedb --tests

  style:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: cargo fmt
        run: cargo fmt -- --check

  flatbuffers:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Install specific version of flatc (24.3.25)
      - name: Install flatc
        run: |
          wget https://github.com/google/flatbuffers/releases/download/v24.3.25/Linux.flatc.binary.g++-13.zip
          unzip Linux.flatc.binary.g++-13.zip
          sudo mv flatc /usr/local/bin/

      # Check for differences
      - name: Generate and validate files
        run: |
          flatc -o tmp/generated --rust --gen-all schemas/manifest.fbs
          if ! diff -r src/generated/*.rs tmp/generated; then
            echo "Generated files do not match!"
            exit 1
          fi

  tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: taiki-e/install-action@v2
        with:
          tool: nextest@0.9.84
      - name: Run Tests
        run: cargo nextest run --all-features --profile ci

  microbenchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run benchmark
        run: cargo bench -- --output-format bencher | tee output.txt

      - name: Download nightly benchmark data
        uses: actions/cache/restore@v4
        with:
          path: ./microbenchmarks-cache
          key: ${{ runner.os }}-microbenchmarks

      - name: Update benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: cargo bench
          tool: 'cargo'
          output-file-path: output.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          external-data-json-path: ./microbenchmarks-cache/benchmark-data.json
          comment-on-alert: true
          summary-always: true
          max-items-in-chart: 30

  microbenchmark-pprofs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic linux-tools-`uname -r`
          pip install git-filter-repo

      - name: Configure perf
        run: |
          echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
          echo 0 | sudo tee /proc/sys/kernel/kptr_restrict

      - name: Generate pprofs
        env:
          CARGO_PROFILE_BENCH_DEBUG: true
          SLATE_BENCH_PROFILE: true
        run: |
          # Get list of benchmarks
          BENCHMARKS=$(cargo bench --no-run --message-format=json | jq -r 'select(.profile.test == true) | .target.name' | sort -u)

          # Create pprofs for each benchmark
          for bench in $BENCHMARKS; do
            cargo bench --bench $bench -- --profile-time 10
          done

      - name: Checkout website repository
        uses: actions/checkout@v4
        with:
          repository: slatedb/slatedb-website
          token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          path: slatedb-website
          ref: gh-pages

      - name: Update website with new pprofs
        env:
          PPROF_WEBSITE_DIR: performance/microbenchmark-pprofs/main
        run: |
          cd slatedb-website
          git config --global user.name 'github-actions[bot]'
          git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
          rm -rf $PPROF_WEBSITE_DIR
          mkdir -p $PPROF_WEBSITE_DIR
          # Copy pprof to $PPROF_WEBSITE_DIR/<benchmark_name>.pb
          find ../target/criterion -name "profile.pb" -exec sh -c 'cp "$1" "$PPROF_WEBSITE_DIR/$(basename $(dirname $(dirname "$1"))).pb"' sh {} \;
          git add $PPROF_WEBSITE_DIR
          git commit -m "Update microbenchmark pprof for job id ${GITHUB_RUN_ID}"
          git push origin gh-pages --force
