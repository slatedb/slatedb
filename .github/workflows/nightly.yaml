name: Nightly Benchmarks

on:
  schedule:
    # Run at midnight Pacific (8 AM UTC)
    - cron: '0 8 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  # Required for storing benchmark results
  contents: write
  # Required for reading ZeroFS's main action status
  actions: read

jobs:
  # Run and save nightly microbenchmark data so PRs have a fresh baseline to compare against
  microbenchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run microbenchmark
        run: cargo bench -- --output-format bencher | tee output.txt

      - name: Download nightly microbenchmark data
        uses: actions/cache/restore@v4
        with:
          path: ./microbenchmarks-cache
          key: ${{ runner.os }}-microbenchmarks-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-microbenchmarks-

      - name: Update microbenchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: cargo bench
          tool: 'cargo'
          output-file-path: output.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          external-data-json-path: ./microbenchmarks-cache/benchmark-data.json
          fail-on-alert: true
          summary-always: true
          max-items-in-chart: 30

      - name: Save nightly microbenchmark data
        uses: actions/cache/save@v4
        with:
          path: ./microbenchmarks-cache
          key: ${{ runner.os }}-microbenchmarks-${{ github.run_id }}

  benchmarks:
    runs-on: warp-ubuntu-latest-x64-16x
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y traceroute
          sudo snap install aws-cli --classic

      - name: System information
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.TIGRIS_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.TIGRIS_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET: ${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}
          AWS_REGION: auto
          AWS_ENDPOINT: https://t3.storage.dev
        run: |
          echo "=== CPU ==="
          lscpu
          echo -e "\n=== Memory ==="
          free -h
          echo -e "\n=== Disk Space ==="
          df -h
          echo -e "\n=== Workspace Directory ==="
          du -sh ${{ github.workspace }}
          echo -e "\n=== Network ==="
          traceroute t3.storage.dev
          echo -e "Generating 1 gig file"
          dd if=/dev/urandom of=/tmp/1gig bs=1G count=1
          echo -e "Uploading 1 gig file"
          time aws s3 cp --endpoint-url $AWS_ENDPOINT /tmp/1gig s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig
          echo -e "Downloading 1 gig file"
          time aws s3 cp --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig /tmp/1gig
          echo -e "Deleting 1 gig file"
          time aws s3 rm --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig

      - name: Download previous mermaid plots
        uses: actions/cache/restore@v4
        with:
          path: ./target/bencher/results/mermaid
          key: ${{ runner.os }}-mermaid-plots-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-mermaid-plots-

      - name: Run benchmark
        env:
          CLOUD_PROVIDER: aws
          AWS_ACCESS_KEY_ID: ${{ secrets.TIGRIS_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.TIGRIS_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET: ${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}
          AWS_REGION: auto
          AWS_ENDPOINT: https://t3.storage.dev
          SLATEDB_BENCH_CLEAN: true
          RUST_LOG: info
        run: |
          aws s3 rm --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }} --recursive
          ./slatedb-bencher/benchmark-db.sh

      - name: Save mermaid plots cache
        uses: actions/cache/save@v4
        with:
          path: ./target/bencher/results/mermaid
          key: ${{ runner.os }}-mermaid-plots-${{ github.run_id }}

      - name: Add mermaid diagrams to summary
        run: |
          echo "# SlateDB Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add each mermaid diagram to the summary
          for mermaid_file in target/bencher/results/mermaid/*.mermaid; do
            if [ -f "$mermaid_file" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo '```mermaid' >> $GITHUB_STEP_SUMMARY
              cat "$mermaid_file" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "Mermaid diagrams added to GitHub Actions summary!"
          echo "Total diagrams: $(ls -1 target/bencher/results/mermaid/*.mermaid 2>/dev/null | wc -l)"

  microbenchmark-pprofs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic linux-tools-`uname -r`

      - name: Configure perf
        run: |
          echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
          echo 0 | sudo tee /proc/sys/kernel/kptr_restrict

      - name: Generate pprofs
        env:
          CARGO_PROFILE_BENCH_DEBUG: true
          SLATE_BENCH_PROFILE: true
        run: |
          BENCHMARKS=$(cargo bench --no-run --message-format=json | jq -r 'select(.profile.test == true and (.target.kind | contains(["bench"]))) | .target.name' | sort -u)
          for bench in $BENCHMARKS; do
            cargo bench --bench $bench -- --profile-time 10
          done
          find . -name "*.pb" -exec gzip {} \;

      # install instructions from https://github.com/polarsignals/pprofme/tree/main
      - name: Install pprofme
        run: |
          curl -LO https://github.com/polarsignals/pprofme/releases/latest/download/pprofme_$(uname)_$(uname -m)
          curl -sL https://github.com/polarsignals/pprofme/releases/latest/download/pprofme_checksums.txt | shasum --ignore-missing -a 256 --check
          chmod a+x pprofme_$(uname)_$(uname -m)
          sudo mv pprofme_$(uname)_$(uname -m) /usr/local/bin/pprofme

      - name: Upload pprofs and add links to summary
        run: |
          echo "## ðŸ”¥ Microbenchmark Performance Profiles" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following pprof profiles were generated from nightly microbenchmarks:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          PPROF_FILES=$(find . -name "*.pb.gz" -type f 2>/dev/null || true)
          if [ -z "$PPROF_FILES" ]; then
            echo "âš ï¸ No pprof files found" >> $GITHUB_STEP_SUMMARY
          else
            for pprof_file in $PPROF_FILES; do
              bench_name=$(basename "$pprof_file" .pb.gz)
              description="SlateDB Nightly Microbenchmark: $bench_name ($(date -u +%Y-%m-%d))"
              upload_url=$(pprofme upload -d "$description" "$pprof_file" 2>&1 | grep -o 'https://pprof.me/[a-zA-Z0-9]*' | head -1)
              if [ -n "$upload_url" ]; then
                echo "- [$bench_name]($upload_url)" >> $GITHUB_STEP_SUMMARY
              else
                echo "- âŒ Failed to upload $bench_name" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

  deterministic-simulation-test:
    runs-on: warp-ubuntu-latest-x64-16x
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: taiki-e/install-action@v2
        with:
          tool: nextest@0.9.98
      - name: Install gdb
        run: |
          sudo apt-get update
          sudo apt-get install -y gdb
      - name: Run DST Tests
        run: cargo nextest run test_dst_nightly -p slatedb-dst --all-features --profile dst-nightly --no-capture
        env:
          RUSTFLAGS: "--cfg dst --cfg tokio_unstable --cfg slow"
          RUST_LOG: "info"
          SLATEDB_DST_ROOT: "./"

  detect-flaky-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4
      - uses: taiki-e/install-action@v2
        with:
          tool: nextest@0.9.98
      - name: Install gdb
        run: |
          sudo apt-get update
          sudo apt-get install -y gdb
      - name: Run tests in a loop for 15 minutes
        run: |
          set -euo pipefail
          SECONDS=0
          deadline=$((15 * 60))
          iteration=1
          while [ $SECONDS -lt $deadline ]; do
            echo "=== Iteration $iteration (elapsed ${SECONDS}s) ==="
            if ! cargo nextest run --workspace --lib --all-features --all-targets --profile ci; then
              echo "Tests failed during iteration $iteration"
              exit 1
            fi
            iteration=$((iteration + 1))
          done
          completed=$((iteration - 1))
          echo "Completed $completed iteration(s) in ${SECONDS}s without failures"

  toxiproxy-chaos:
    name: Toxiproxy Chaos
    runs-on: ubuntu-latest
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli jq curl

      - name: Build test (compile only)
        run: |
          cargo test -p slatedb --test db --no-run

      - name: Create docker network
        run: docker network create chaos-net

      - name: Start MinIO
        run: |
          docker run -d --name minio --network chaos-net \
            -p 9000:9000 -p 9001:9001 \
            -e MINIO_ROOT_USER=minioadmin \
            -e MINIO_ROOT_PASSWORD=minioadmin \
            minio/minio server /data --console-address :9001
          # Wait for readiness
          for i in {1..60}; do
            if curl -sf http://127.0.0.1:9000/minio/health/ready >/dev/null; then echo ready; break; fi; sleep 1; done

      - name: Start Envoy
        run: |
          docker run -d --name envoy --network chaos-net \
            -p 8080:8080 -p 9901:9901 \
            -v "${{ github.workspace }}/scripts/envoy-s3.yaml:/etc/envoy/envoy.yaml:ro" \
            envoyproxy/envoy:latest -c /etc/envoy/envoy.yaml
          # Wait for admin
          for i in {1..60}; do
            if curl -sf http://127.0.0.1:9901/server_info >/dev/null; then echo ready; break; fi; sleep 1; done

      - name: Start Toxiproxy
        run: |
          docker run -d --name toxiproxy --network chaos-net \
            -p 8474:8474 -p 9001:9001 \
            ghcr.io/shopify/toxiproxy:latest
          # Wait for API
          for i in {1..60}; do
            if curl -sf http://127.0.0.1:8474/proxies >/dev/null; then echo ready; break; fi; sleep 1; done

      - name: Create S3 bucket (through proxy)
        env:
          AWS_ACCESS_KEY_ID: minioadmin
          AWS_SECRET_ACCESS_KEY: minioadmin
          AWS_REGION: us-east-1
        run: |
          # Create proxy to Envoy (if not exists)
          curl -sf -X POST http://127.0.0.1:8474/proxies \
            -H 'Content-Type: application/json' \
            -d '{"name":"s3","listen":"0.0.0.0:9001","upstream":"envoy:8080"}' || true
          # Create bucket via the proxied endpoint
          aws --endpoint-url http://127.0.0.1:9001 s3api create-bucket --bucket slatedb-test || true

      - name: Run chaos scenarios
        id: chaos
        env:
          SLATEDB_TEST_NUM_WRITERS: "12"
          SLATEDB_TEST_NUM_READERS: "3"
          SLATEDB_TEST_WRITES_PER_TASK: "1000000"
          SLATEDB_TEST_KEY_LENGTH: "256"
        run: |
          chmod +x scripts/run_chaos_scenarios.sh
          if scripts/run_chaos_scenarios.sh; then
            echo "ok=true" >> $GITHUB_OUTPUT
          else
            echo "ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Summarize results
        if: always()
        run: |
          echo "## ðŸŒªï¸ SlateDB Toxiproxy Chaos" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.chaos.outputs.ok }}" = "true" ]; then
            echo "All scenarios passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "One or more scenarios failed" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Dump logs on failure
        if: failure()
        run: |
          echo "--- MinIO logs ---"; docker logs --tail 500 minio || true
          echo "--- Envoy logs ---"; docker logs --tail 500 envoy || true
          echo "--- Toxiproxy logs ---"; docker logs --tail 500 toxiproxy || true
          echo "--- Envoy runtime ---"; curl -sf http://127.0.0.1:9901/runtime || true
          echo "--- Toxiproxy proxies ---"; curl -sf http://127.0.0.1:8474/proxies || true
