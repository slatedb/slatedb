name: Nightly Benchmarks

on:
  schedule:
    # Run at midnight Pacific (8 AM UTC)
    - cron: '0 8 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  # Required for storing benchmark results
  contents: write
  # Required for reading ZeroFS's main action status
  actions: read

jobs:
  # Run and save nightly microbenchmark data so PRs have a fresh baseline to compare against
  microbenchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run microbenchmark
        run: cargo bench -- --output-format bencher | tee output.txt

      - name: Download nightly microbenchmark data
        uses: actions/cache/restore@v4
        with:
          path: ./microbenchmarks-cache
          key: ${{ runner.os }}-microbenchmarks-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-microbenchmarks-

      - name: Update microbenchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: cargo bench
          tool: 'cargo'
          output-file-path: output.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          external-data-json-path: ./microbenchmarks-cache/benchmark-data.json
          fail-on-alert: true
          summary-always: true
          max-items-in-chart: 30

      - name: Save nightly microbenchmark data
        uses: actions/cache/save@v4
        with:
          path: ./microbenchmarks-cache
          key: ${{ runner.os }}-microbenchmarks-${{ github.run_id }}

  benchmarks:
    runs-on: warp-ubuntu-latest-x64-16x
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y traceroute
          sudo snap install aws-cli --classic

      - name: System information
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.TIGRIS_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.TIGRIS_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET: ${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}
          AWS_REGION: auto
          AWS_ENDPOINT: https://t3.storage.dev
        run: |
          echo "=== CPU ==="
          lscpu
          echo -e "\n=== Memory ==="
          free -h
          echo -e "\n=== Disk Space ==="
          df -h
          echo -e "\n=== Workspace Directory ==="
          du -sh ${{ github.workspace }}
          echo -e "\n=== Network ==="
          traceroute t3.storage.dev
          echo -e "Generating 1 gig file"
          dd if=/dev/urandom of=/tmp/1gig bs=1G count=1
          echo -e "Uploading 1 gig file"
          time aws s3 cp --endpoint-url $AWS_ENDPOINT /tmp/1gig s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig
          echo -e "Downloading 1 gig file"
          time aws s3 cp --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig /tmp/1gig
          echo -e "Deleting 1 gig file"
          time aws s3 rm --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}/1gig

      - name: Download previous mermaid plots
        uses: actions/cache/restore@v4
        with:
          path: ./target/bencher/results/mermaid
          key: ${{ runner.os }}-mermaid-plots-${{ github.run_id }}
          restore-keys: |
            ${{ runner.os }}-mermaid-plots-

      - name: Run benchmark
        env:
          CLOUD_PROVIDER: aws
          AWS_ACCESS_KEY_ID: ${{ secrets.TIGRIS_AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.TIGRIS_AWS_SECRET_ACCESS_KEY }}
          AWS_BUCKET: ${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }}
          AWS_REGION: auto
          AWS_ENDPOINT: https://t3.storage.dev
          SLATEDB_BENCH_CLEAN: true
        run: |
          aws s3 rm --endpoint-url $AWS_ENDPOINT s3://${{ secrets.TIGRIS_AWS_BUCKET_BENCHER }} --recursive
          ./slatedb-bencher/benchmark-db.sh

      - name: Save mermaid plots cache
        uses: actions/cache/save@v4
        with:
          path: ./target/bencher/results/mermaid
          key: ${{ runner.os }}-mermaid-plots-${{ github.run_id }}

      - name: Add mermaid diagrams to summary
        run: |
          echo "# SlateDB Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add each mermaid diagram to the summary
          for mermaid_file in target/bencher/results/mermaid/*.mermaid; do
            if [ -f "$mermaid_file" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo '```mermaid' >> $GITHUB_STEP_SUMMARY
              cat "$mermaid_file" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "Mermaid diagrams added to GitHub Actions summary!"
          echo "Total diagrams: $(ls -1 target/bencher/results/mermaid/*.mermaid 2>/dev/null | wc -l)"

  microbenchmark-pprofs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic linux-tools-`uname -r`

      - name: Configure perf
        run: |
          echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
          echo 0 | sudo tee /proc/sys/kernel/kptr_restrict

      - name: Generate pprofs
        env:
          CARGO_PROFILE_BENCH_DEBUG: true
          SLATE_BENCH_PROFILE: true
        run: |
          BENCHMARKS=$(cargo bench --no-run --message-format=json | jq -r 'select(.profile.test == true and (.target.kind | contains(["bench"]))) | .target.name' | sort -u)
          for bench in $BENCHMARKS; do
            cargo bench --bench $bench -- --profile-time 10
          done
          find . -name "*.pb" -exec gzip {} \;

      # install instructions from https://github.com/polarsignals/pprofme/tree/main
      - name: Install pprofme
        run: |
          curl -LO https://github.com/polarsignals/pprofme/releases/latest/download/pprofme_$(uname)_$(uname -m)
          curl -sL https://github.com/polarsignals/pprofme/releases/latest/download/pprofme_checksums.txt | shasum --ignore-missing -a 256 --check
          chmod a+x pprofme_$(uname)_$(uname -m)
          sudo mv pprofme_$(uname)_$(uname -m) /usr/local/bin/pprofme

      - name: Upload pprofs and add links to summary
        run: |
          echo "## 🔥 Microbenchmark Performance Profiles" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The following pprof profiles were generated from nightly microbenchmarks:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          PPROF_FILES=$(find . -name "*.pb.gz" -type f 2>/dev/null || true)
          if [ -z "$PPROF_FILES" ]; then
            echo "⚠️ No pprof files found" >> $GITHUB_STEP_SUMMARY
          else
            for pprof_file in $PPROF_FILES; do
              bench_name=$(basename "$pprof_file" .pb.gz)
              description="SlateDB Nightly Microbenchmark: $bench_name ($(date -u +%Y-%m-%d))"
              upload_url=$(pprofme upload -d "$description" "$pprof_file" 2>&1 | grep -o 'https://pprof.me/[a-zA-Z0-9]*' | head -1)
              if [ -n "$upload_url" ]; then
                echo "- [$bench_name]($upload_url)" >> $GITHUB_STEP_SUMMARY
              else
                echo "- ❌ Failed to upload $bench_name" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

  deterministic-simulation-test:
    runs-on: warp-ubuntu-latest-x64-16x
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: taiki-e/install-action@v2
        with:
          tool: nextest@0.9.98
      - name: Install gdb
        run: |
          sudo apt-get update
          sudo apt-get install -y gdb
      - name: Run DST Tests
        run: cargo nextest run test_dst_nightly -p slatedb-dst --all-features --profile dst-nightly --no-capture
        env:
          RUSTFLAGS: "--cfg dst --cfg tokio_unstable --cfg slow"
          RUST_LOG: "warn"
          SLATEDB_DST_ROOT: "./"

  notify:
    needs: [microbenchmarks, benchmarks, microbenchmark-pprofs, deterministic-simulation-test]
    if: ${{ always() }}
    permissions:
      issues: write
      contents: read
    runs-on: ubuntu-latest
    steps:
      - name: Comment or open issue if any dependency failed
        if: ${{ contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled') }}
        uses: actions/github-script@v7
        with:
          script: |
            const {owner, repo} = context.repo;
            const runUrl = `${context.serverUrl}/${owner}/${repo}/actions/runs/${context.runId}`;
            const body = `Another failure.\n\nRun: ${runUrl}`;

            const issues = await github.paginate(
              github.rest.issues.listForRepo,
              { owner, repo, state: 'open', per_page: 100 }
            );
            const hit = issues.find(i => i.title === 'Nightly failed');

            if (hit) {
              await github.rest.issues.createComment({ owner, repo, issue_number: hit.number, body });
            } else {
              await github.rest.issues.create({
                owner, repo,
                title: 'Nightly failed',
                body: `Run: ${runUrl}`,
                labels: ['ci-failure']
              });
            }

      - name: 'Close any open "Nightly failed" issues on success'
        if: ${{ !contains(needs.*.result, 'failure') && !contains(needs.*.result, 'cancelled') }}
        uses: actions/github-script@v7
        with:
          script: |
            const {owner, repo} = context.repo;
            const runUrl = `${context.serverUrl}/${owner}/${repo}/actions/runs/${context.runId}`;
            const body = `Nightly succeeded.\n\nRun: ${runUrl}\n\nClosing this issue.`;

            const issues = await github.paginate(
              github.rest.issues.listForRepo,
              { owner, repo, state: 'open', per_page: 100 }
            );
            const toClose = issues.filter(i => i.title === 'Nightly failed');

            for (const issue of toClose) {
              await github.rest.issues.createComment({ owner, repo, issue_number: issue.number, body });
              await github.rest.issues.update({ owner, repo, issue_number: issue.number, state: 'closed' });
            }
